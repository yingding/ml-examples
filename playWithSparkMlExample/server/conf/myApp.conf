##
## This file needs to referenced by the shell variable CONFIG
##
##
## String shall be used without the space and quotes
app.tcp.seed=seedToChange
##
## Daemon Conf
##
## false / true
app.exampleDaemon.isActivated=false
app.exampleDaemon.delay=10
## InSeconds = every minutes
app.exampleDaemon.executionInterval=60

## Spark ML config
# shall the spark ml module be activated
# false / true
app.spark.isActivated=true
# Spark Cluster Session Config Path
# app.spark.configPath=/

# spark work lives in bridge Network Container, which can only communicate with gateway
# CASE 1: mongodb container is in the same docker bridge network as spark cluster, use the -link name of mongodb container
# CASE 2: mongodb container is outside the bridge network of spark cluster, use the IP address of mongodb container
# Notice: In CASE 2 "127.0.0.1", "Gateway IP of bridge network", "host.docker.internal" will all NOT work.
app.spark.mongoHost=yourHostIp
app.spark.mongoPort=27017
app.spark.mongoUser=xxx
app.spark.mongoPW=yyy
app.spark.mongoIsReplicaSet=false
# monogReplicaSetName will be used if only mongoIsReplicaSet is true, otherwise it will be ignored
app.spark.monogReplicaSetName=zzzz
# Not used any more
# https://stackoverflow.com/questions/43417216/spark-submit-packages-is-not-working-on-my-cluster-what-could-be-the-reason
# app.spark.jarsPackages=mongo-spark-connector_2.12

## Spark Cluster Config params ##
app.spark.driver.port="7070|7078"
# driver.host shall either be gateway ip for linux docker, or host.docker.internal for windows/macosx docker
# app.spark.driver.host=host.docker.internal
app.spark.driver.host="BrigeNetworkGateWay|host.docker.internal"
# driver.bindAddress uses en10 ip // 127.0.0.1 will resolve to my local interface en10
app.spark.driver.bindAddress=127.0.0.1
# port for blockManager
app.spark.driver.blockManager.port=10026






